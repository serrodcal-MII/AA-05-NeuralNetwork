{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales (ejercicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autor**: Sergio Rodríguez Calvo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comenzar con las importaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importante: comentar adecuadamente cada paso realizado**, relacionándolo con lo visto en la teoría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: aplicación de redes neuronales a clasificación (análisis de sentimientos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pide aplicar un modelo de redes neuronales al problema de decidir si una crítica de cine es positiva o negativa. Para ello volvemos a usar los datos de IMDB (Internet Movie Database) que vimos en el módulo 2 (modelo probabilístico).\n",
    "\n",
    "Hacerlo usando los dos sistemas vistos, comparando los resultados:\n",
    "* Scikit learn: usar `MLPClassifier`\n",
    "* Keras con Tensorflow: usar `Sequential` y capas tipo `Dense` con la arquitectura adecuda.\n",
    "\n",
    "\n",
    "Aunque ya hemos visto que los datos están disponibles en http://ai.stanford.edu/~amaas/data/sentiment/ , en este caso pedimos cargar los datos usando la utilidad `imdb`de Keras. Se puede consultar en el manual de Keras: https://keras.io/datasets/ Cargarlos con `imdb.load_data` y usar los datos cargados como punto de partida a este ejercicio (tanto para su aplicar scikit learn como para aplicar keras). Prestar atención al formato en el que se cargar, que no es el mismo que hamos visto hasta ahora.  \n",
    "\n",
    "Los textos han de ser vectorizados para que se puedan ser procesados por una red. Para esto, tenemos varias alternativas, usar una de ellas:\n",
    "\n",
    "* Vectorizando \"manualmente\", definiendo una función en python que lo haga.\n",
    "* Vectorizadores de scikit learn (ya vistos)\n",
    "* Herramientas de vectorización de keras: https://keras.io/preprocessing/text/\n",
    "\n",
    "Mostrar algunas pruebas realizadas con distintas arquitecturas y/o hiperparámetros. No es necesario ser muy exhaustivo ni usar `GridSearchCV` en scikit learn ni el equivalente en keras. Tan solo mostrar alguna experimentación y ajuste manual.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a cargar el _dataset_ de críticas de cine a partir de la librería Keras, para ello usaremos la función `load_data` que nos devuelve los conjuntos separados en entrenamiento y pruebas, así como, sus clasificaciones respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver la forma de los datos para ambos conjuntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada conjunto cuenta con 25.000 ejemplos, en este caso, con una sola columna correspondiente a el texto con la crítica. Además, el número de columnas es variable, por eso no sale información, por lo que vamos a necesitar de un preprocesado para tener el mismo número de columnas en todas las instancias. Ahora, vamos a echar un vistazo a una de las instancias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 13, 244, 6, 87, 337, 7, 628, 2219, 5, 28, 285, 15, 240, 93, 23, 288, 549, 18, 1455, 673, 4, 241, 534, 3635, 8448, 20, 38, 54, 13, 258, 46, 44, 14, 13, 1241, 7258, 12, 5, 5, 51, 9, 14, 45, 6, 762, 7, 17802, 1309, 328, 5, 428, 2473, 15, 26, 1292, 5, 3939, 6728, 5, 1960, 279, 13, 92, 124, 803, 52, 21, 279, 14, 9, 43, 6, 762, 7, 595, 15, 16, 28911, 23, 4, 1071, 467, 4, 403, 7, 628, 2219, 8, 97, 6, 171, 3596, 99, 387, 72, 97, 12, 788, 15, 13, 161, 459, 44, 4, 3939, 1101, 173, 21, 69, 8, 401, 22239, 4, 481, 88, 61, 4731, 238, 28, 32, 11, 32, 14, 9, 6, 545, 1332, 766, 5, 203, 73, 28, 43, 77, 317, 11, 4, 22228, 953, 270, 17, 6, 3616, 13, 545, 386, 25, 92, 1142, 129, 278, 23, 14, 241, 46, 7, 158]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se comenta en la descripción, las críticas o revisiones han sido preprocesadas por lo que cada crítica está codificada como una lista de palabras indexadas (enteros). Esto es, cada palabra está codificada por la frecuencia sobre el dataset al completo (si el entero para esa palabra es 3, eso significa que es la tercera palabra más frecuente).\n",
    "\n",
    "Esto permite un filtrado rapido de tipo \"elimina las 20 palabras más comunes\". Al cargar los datos, existe un parámetro `skip_top` que permite realizar este proceso de manera automática mientras se realiza la carga. En este experimento, por tanto, vamos a evitar filtrado alguno ya que se ha usado el parámetro por defecto a 0, y vamos a utilizar los datos con la vectorización por defecto, la que acabamos de describir.\n",
    "\n",
    "Además, los datos vienen clasificados como positivos (0) o negativos (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a obtener el mensaje original para aplicar una vectorización a los mensajes como se ha visto en ejercicios previos. Para ello, vamos a ser uso del índice de palabras del dataset, vamos a crear un diccionario para poder realizar este proceso a partir de los indices de las palabras y, finalmente, vamos a crear una función que decodifique cada crítica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of crudely drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was foisted on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep adjusting the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the deluxe box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\n"
     ]
    }
   ],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "rev_word_index = {idx:w for w,idx in word_index.items()}\n",
    "\n",
    "def decode_sentence(s):\n",
    "    # index 0 to 2 are reserved for things like padding, unknown word, etc.\n",
    "    decoded_sent = [rev_word_index.get(idx-3, '') for idx in s]\n",
    "    return ' '.join(decoded_sent)\n",
    "\n",
    "print(decode_sentence(x_train[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, hemos mostrado una crítica del conjunto de entrenamiento, la misma que mostramos anteriormente como vector de números.\n",
    "\n",
    "A continuación, vamos a decodificar todos los mensajes y a obtener un conjunto de entrenamiento decodificado, así como un conjunto de pruebas decodificado. Para ello, vamos a usar la función `decode_sentence` definida previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of crudely drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was foisted on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep adjusting the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the deluxe box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\n"
     ]
    }
   ],
   "source": [
    "x_train_decode = []\n",
    "for sentence in x_train:\n",
    "    decode_sent = decode_sentence(sentence)\n",
    "    x_train_decode.append(decode_sent)\n",
    "\n",
    "print(x_train_decode[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a quick glance at the premise of this film would seem to indicate just another dumb '80's inbred backwood slash fest the type where sex equals death and the actors are all annoying stereotypes you actually want to die however delivers considerably more br br rather than focus on bare flesh and gore though there is a little of each no sex however the flick focuses on delivering impending dread mounting tension amidst a lovely scenic backdrop these feelings are further heightened by a cast of realistically likable characters and antagonists that are more amoral than cardboard definitions of evil oh yeah george kennedy is here too and when is that not a good thing br br if you liked wrong turn then watch this to see where much of its' methodology came from\n"
     ]
    }
   ],
   "source": [
    "x_test_decode = []\n",
    "for sentence in x_test:\n",
    "    decode_sent = decode_sentence(sentence)\n",
    "    x_test_decode.append(decode_sent)\n",
    "\n",
    "print(x_test_decode[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es vectorizar los conjuntos de entrenamiento y pruebas, para ello, vamos a importar y utilizar `CountVectorizer` al igual que hicimos en ejercicios anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(x_train_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado el vectorizador, procedemos a obtener los conjuntos de entrenamiento y pruebas vectorizado. También, vamos a indicar las dimensiones de la matriz dispersa obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74702 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3445804 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "X_train = vect.transform(x_train_decode)\n",
    "X_test = vect.transform(x_test_decode)\n",
    "\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de entrenamiento tiene tantas filas como críticas o revisiones habia en el conjunto original y cuenta con 74.702 componentes, todos del mismo tamaño, que no es más que un vector de ceros y unos indicando cual de las palabras del corpus forman parte de dicha instancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, ya podemos comenzar el experimento utilizando `MLPClassifier` de scikit-learn. Para ello, vamos a crear el modelo y a entrenarlo directamente. Vamos a usar los parámetros por defecto salvo los que se modifican en la instancia del modelo, que son:\n",
    "* `solver`: vamos a mantener el mismo algoritmo de optimización de los pesos que en el ejercicio visto en la asignatura, ya que estabamos en un problema de similares características, es decir, clasificación binaria.\n",
    "* `random_state`: ya se ha visto en ejercicios anteriores, es una forma de fijar la inizialización del modelo. De esta forma, siempre obtendremos el mismo resultado. Es algo que usaremos a lo largo de la práctica allí donde se pueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento en entenamiento: 0.99\n",
      "Rendimiento en el conjunto de prueba: 0.86\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print(\"Rendimiento en entenamiento: {:.2f}\".format(mlp.score(X_train, y_train)))\n",
    "print(\"Rendimiento en el conjunto de prueba: {:.2f}\".format(mlp.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a modificar el parámetro `alpha` para ver si obtenemos un resultado diferente del anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento en entenamiento: 0.99\n",
      "Rendimiento en el conjunto de prueba: 0.85\n"
     ]
    }
   ],
   "source": [
    "mlp_alpha_1 = MLPClassifier(solver='lbfgs', alpha=1, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print(\"Rendimiento en entenamiento: {:.2f}\".format(mlp_alpha_1.score(X_train, y_train)))\n",
    "print(\"Rendimiento en el conjunto de prueba: {:.2f}\".format(mlp_alpha_1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido similar resultado con ambos experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a repetir el experimento utilizando un modelo de red neuronal con Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 15:22:04.381745: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar la misma configuración que en el ejercicio de redenes neuronales, y vamos también a crear un conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo configurando 30 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 100), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 4ms/step - loss: 0.5699 - accuracy: 0.7162 - val_loss: 0.6850 - val_accuracy: 0.6396\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.4561 - accuracy: 0.8051 - val_loss: 0.4055 - val_accuracy: 0.8322\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.3999 - accuracy: 0.8343 - val_loss: 0.3626 - val_accuracy: 0.8530\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.3640 - accuracy: 0.8501 - val_loss: 0.3522 - val_accuracy: 0.8532\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.3293 - accuracy: 0.8648 - val_loss: 0.3238 - val_accuracy: 0.8694\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3190 - accuracy: 0.8725 - val_loss: 0.3224 - val_accuracy: 0.8686\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2910 - accuracy: 0.8840 - val_loss: 0.3055 - val_accuracy: 0.8724\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2811 - accuracy: 0.8892 - val_loss: 0.3076 - val_accuracy: 0.8718\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2707 - accuracy: 0.8931 - val_loss: 0.3001 - val_accuracy: 0.8788\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2511 - accuracy: 0.9002 - val_loss: 0.4437 - val_accuracy: 0.8244\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2434 - accuracy: 0.9038 - val_loss: 0.3305 - val_accuracy: 0.8618\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2271 - accuracy: 0.9105 - val_loss: 0.2911 - val_accuracy: 0.8806\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2220 - accuracy: 0.9127 - val_loss: 0.6166 - val_accuracy: 0.7786\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2058 - accuracy: 0.9178 - val_loss: 0.3628 - val_accuracy: 0.8588\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1989 - accuracy: 0.9228 - val_loss: 0.3427 - val_accuracy: 0.8622\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2006 - accuracy: 0.9225 - val_loss: 0.3250 - val_accuracy: 0.8774\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1940 - accuracy: 0.9273 - val_loss: 0.2958 - val_accuracy: 0.8812\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1745 - accuracy: 0.9327 - val_loss: 0.4204 - val_accuracy: 0.8354\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1666 - accuracy: 0.9355 - val_loss: 0.2890 - val_accuracy: 0.8852\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1662 - accuracy: 0.9377 - val_loss: 0.3061 - val_accuracy: 0.8780\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1449 - accuracy: 0.9466 - val_loss: 0.3290 - val_accuracy: 0.8736\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1352 - accuracy: 0.9505 - val_loss: 0.3564 - val_accuracy: 0.8720\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1475 - accuracy: 0.9488 - val_loss: 0.4494 - val_accuracy: 0.8402\n",
      "Epoch 24/30\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1260 - accuracy: 0.9531 - val_loss: 0.3164 - val_accuracy: 0.8800\n",
      "Epoch 25/30\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1258 - accuracy: 0.9583 - val_loss: 0.3336 - val_accuracy: 0.8762\n",
      "Epoch 26/30\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1098 - accuracy: 0.9621 - val_loss: 0.3606 - val_accuracy: 0.8700\n",
      "Epoch 27/30\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0989 - accuracy: 0.9676 - val_loss: 0.3723 - val_accuracy: 0.8750\n",
      "Epoch 28/30\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1089 - accuracy: 0.9643 - val_loss: 0.5097 - val_accuracy: 0.8546\n",
      "Epoch 29/30\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1035 - accuracy: 0.9668 - val_loss: 0.4289 - val_accuracy: 0.8704\n",
      "Epoch 30/30\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.1043 - accuracy: 0.9694 - val_loss: 0.4699 - val_accuracy: 0.8492\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5164 - accuracy: 0.8332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5163785219192505, 0.8331999778747559]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos como hemos conseguido en el entrenamiento reducir mucho la perdida y subir el _accuracy_, pero al evaliar el modelo sobre el conjunto de pruebas, el _accuracy_ se reduce un poco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: aplicación de redes neuronales a regresión (predicción del precio de la vivienda)\n",
    "\n",
    "Se pide aplicar un modelo de redes neuronales al problema de predecir precios de vivienda usando el conjunto de datos  `Boston house prices`. \n",
    "\n",
    "Hacerlo usando los dos sistemas vistos, comparando los resultados:\n",
    "* Scikit learn: usar `MLPRegressor`\n",
    "* Keras con Tensorflow: usar nuevamente `Sequential` y capas tipo `Dense` con la arquitectura adecuada.\n",
    "\n",
    "El conjunto de datos se puede cargar usando tanto scikit learn (`sklearn.datasets.load_boston`) como keras (`keras.datasets.boston_housing`). \n",
    "\n",
    "Como en la parte 1, se pide mostrar algunas pruebas de los resultados obtenidos usando distintas arquitecturas y/o hiperparámetros. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comenzar obteniendo el conjunto de datos, separado en entrenamiento (70%) y pruebas (30%), desde la librería de _datasets_ de keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
    "    path=\"boston_housing.npz\", test_split=0.3, seed=113\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se componen de 13 caractersticas, lo que necesitaremos tener en cuenta posteriormente para la capa de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos necesitan ser reescalados entre 0 y 1, como vemos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.23247,   0.     ,   8.14   ,   0.     ,   0.538  ,   6.142  ,\n",
       "        91.7    ,   3.9769 ,   4.     , 307.     ,  21.     , 396.9    ,\n",
       "        18.72   ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar la clase `MinMaxScaler`, para escalar los datos entre 0 y 1 antes de comenzar con la ejecución de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(x_train)\n",
    "x_train = mms.transform(x_train)\n",
    "x_test = mms.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos a continuación, los datos están escalados entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01378163, 0.        , 0.28152493, 0.        , 0.31481481,\n",
       "       0.49980635, 0.91452111, 0.29719123, 0.13043478, 0.22753346,\n",
       "       0.89361702, 1.        , 0.51422518])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a entrenar y evaluar el modelo _MLRegresor_. Vamos a establecer un número alto de iteraciones para permitir al modelo converger y evitar el mensaje de falta de convergencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento sobre el conjunto de entrenamiento: 0.94\n",
      "Rendimiento sobre el conjunto de test: 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr1 = MLPRegressor(random_state=113, max_iter=10000).fit(x_train, y_train)\n",
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(regr1.score(x_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(regr1.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar con un alpha mayor que el por defecto, es decir, hemos probado con `alpha=0.01` y vamos a probar ahora con `alpha=0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento sobre el conjunto de entrenamiento: 0.94\n",
      "Rendimiento sobre el conjunto de test: 0.78\n"
     ]
    }
   ],
   "source": [
    "regr2 = MLPRegressor(alpha=0.1, random_state=113, max_iter=10000).fit(x_train, y_train)\n",
    "print(\"Rendimiento sobre el conjunto de entrenamiento: {:.2f}\".format(regr2.score(x_train, y_train)))\n",
    "print(\"Rendimiento sobre el conjunto de test: {:.2f}\".format(regr2.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido el mismo resultado, un rendimiento de 0.94 sobre el conjunto de entrenamiento y 0.78 sobre el de pruebas, lo cual es poco.\n",
    "\n",
    "Ahora, vamos a porbar con un modelo de keras. Vamos a establecer 3 capas, una capa de entrada de 13 neuronas, una por cada carácteristica, dos capas ocultas con función de activación RELU de 300 y 100 neuronas respectivamente y, como estamos en un problema de regresión, vamos a finalizar con una capa de salida con una neurona de tipo lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(300, input_dim = 13, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer algunos cambios respecto al ejercicio proporcionado. Aquí, vamos a utilizar la función de pérdida `mse` (error cuadrático medio), utilizada en modelos de regresión no logistica. Por otro lado, vamos a utilizar la métrica `mae` (error absoluto medio). Finalmente, vamos a utilizar otro algoritmo de optimización, `rmsprop`, que:\n",
    "\n",
    "* Mantiene una media móvil del cuadrado del graciente.\n",
    "* Divide el gradiente entre la raíz de esa media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrenar el modelo, manteiendo 30 epocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 478.0164 - mae: 19.5941\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 245.7596 - mae: 12.6007\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 132.2709 - mae: 8.6734\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 98.6555 - mae: 7.4164\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 80.8217 - mae: 6.5441\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 67.4714 - mae: 5.7948\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 60.2912 - mae: 5.6286\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 52.8140 - mae: 5.0492\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 50.6027 - mae: 5.1114\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 45.8438 - mae: 4.6578\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 43.9634 - mae: 4.6741\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 40.1894 - mae: 4.4823\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 38.6233 - mae: 4.3898\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 35.6119 - mae: 4.1483\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 32.9082 - mae: 3.8945\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 30.4971 - mae: 3.8166\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 27.9678 - mae: 3.6346\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 27.2899 - mae: 3.6409\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 27.0543 - mae: 3.7494\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 24.7338 - mae: 3.3761\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 23.5150 - mae: 3.4062\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 22.8702 - mae: 3.2991\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 22.9367 - mae: 3.2498\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 21.4093 - mae: 3.2557\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 21.8548 - mae: 3.2560\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 21.4892 - mae: 3.2125\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 21.0818 - mae: 3.1723\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 20.3813 - mae: 3.1329\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 20.4466 - mae: 3.1521\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 19.4966 - mae: 3.0705\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a evaluar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 24.5468 - mae: 3.6412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[24.546831130981445, 3.6412293910980225]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos como en cada época ha ido reduciendo la perdida y mejorando la métrica que hemos elegido. Aunque en las últimas épocas la mejora ha sido muy reducida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos a realizar una predicción sobre el tercer elemento del conjunto de test y a compararlo con la salida del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediccion:  24.493797\n",
      "Valor real:  23.9\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(x_test[2:3])\n",
    "print(\"Prediccion: \", predict[0][0])\n",
    "print(\"Valor real: \", y_test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos como el valor predicho por el modelo es prácticamente igual al valor real."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
