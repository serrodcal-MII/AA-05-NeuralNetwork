{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales (ejercicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autor**: Sergio Rodríguez Calvo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comenzar con las importaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importante: comentar adecuadamente cada paso realizado**, relacionándolo con lo visto en la teoría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: aplicación de redes neuronales a clasificación (análisis de sentimientos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pide aplicar un modelo de redes neuronales al problema de decidir si una crítica de cine es positiva o negativa. Para ello volvemos a usar los datos de IMDB (Internet Movie Database) que vimos en el módulo 2 (modelo probabilístico).\n",
    "\n",
    "Hacerlo usando los dos sistemas vistos, comparando los resultados:\n",
    "* Scikit learn: usar `MLPClassifier`\n",
    "* Keras con Tensorflow: usar `Sequential` y capas tipo `Dense` con la arquitectura adecuda.\n",
    "\n",
    "\n",
    "Aunque ya hemos visto que los datos están disponibles en http://ai.stanford.edu/~amaas/data/sentiment/ , en este caso pedimos cargar los datos usando la utilidad `imdb`de Keras. Se puede consultar en el manual de Keras: https://keras.io/datasets/ Cargarlos con `imdb.load_data` y usar los datos cargados como punto de partida a este ejercicio (tanto para su aplicar scikit learn como para aplicar keras). Prestar atención al formato en el que se cargar, que no es el mismo que hamos visto hasta ahora.  \n",
    "\n",
    "Los textos han de ser vectorizados para que se puedan ser procesados por una red. Para esto, tenemos varias alternativas, usar una de ellas:\n",
    "\n",
    "* Vectorizando \"manualmente\", definiendo una función en python que lo haga.\n",
    "* Vectorizadores de scikit learn (ya vistos)\n",
    "* Herramientas de vectorización de keras: https://keras.io/preprocessing/text/\n",
    "\n",
    "Mostrar algunas pruebas realizadas con distintas arquitecturas y/o hiperparámetros. No es necesario ser muy exhaustivo ni usar `GridSearchCV` en scikit learn ni el equivalente en keras. Tan solo mostrar alguna experimentación y ajuste manual.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a cargar el _dataset_ de críticas de cine a partir de la librería Keras, para ello usaremos la función `load_data` que nos devuelve los conjuntos separados en entrenamiento y pruebas, así como, sus clasificaciones respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver la forma de los datos para ambos conjuntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada conjunto cuenta con 25.000 ejemplos, en este caso, con una sola columna correspondiente a el texto con la crítica. Además, el número de columnas es variable, por eso no sale información, por lo que vamos a necesitar de un preprocesado para tener el mismo número de columnas en todas las instancias. Ahora, vamos a echar un vistazo a una de las instancias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 13, 244, 6, 87, 337, 7, 628, 2219, 5, 28, 285, 15, 240, 93, 23, 288, 549, 18, 1455, 673, 4, 241, 534, 3635, 8448, 20, 38, 54, 13, 258, 46, 44, 14, 13, 1241, 7258, 12, 5, 5, 51, 9, 14, 45, 6, 762, 7, 17802, 1309, 328, 5, 428, 2473, 15, 26, 1292, 5, 3939, 6728, 5, 1960, 279, 13, 92, 124, 803, 52, 21, 279, 14, 9, 43, 6, 762, 7, 595, 15, 16, 28911, 23, 4, 1071, 467, 4, 403, 7, 628, 2219, 8, 97, 6, 171, 3596, 99, 387, 72, 97, 12, 788, 15, 13, 161, 459, 44, 4, 3939, 1101, 173, 21, 69, 8, 401, 22239, 4, 481, 88, 61, 4731, 238, 28, 32, 11, 32, 14, 9, 6, 545, 1332, 766, 5, 203, 73, 28, 43, 77, 317, 11, 4, 22228, 953, 270, 17, 6, 3616, 13, 545, 386, 25, 92, 1142, 129, 278, 23, 14, 241, 46, 7, 158]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se comenta en la descripción, las críticas o revisiones han sido preprocesadas por lo que cada crítica está codificada como una lista de palabras indexadas (enteros). Esto es, cada palabra está codificada por la frecuencia sobre el dataset al completo (si el entero para esa palabra es 3, eso significa que es la tercera palabra más frecuente).\n",
    "\n",
    "Esto permite un filtrado rapido de tipo \"elimina las 20 palabras más comunes\". Al cargar los datos, existe un parámetro `skip_top` que permite realizar este proceso de manera automática mientras se realiza la carga. En este experimento, por tanto, vamos a evitar filtrado alguno ya que se ha usado el parámetro por defecto a 0, y vamos a utilizar los datos con la vectorización por defecto, la que acabamos de describir.\n",
    "\n",
    "Además, los datos vienen clasificados como positivos (0) o negativos (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a obtener el mensaje original para aplicar una vectorización a los mensajes como se ha visto en ejercicios previos. Para ello, vamos a ser uso del índice de palabras del dataset, vamos a crear un diccionario para poder realizar este proceso a partir de los indices de las palabras y, finalmente, vamos a crear una función que decodifique cada crítica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of crudely drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was foisted on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep adjusting the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the deluxe box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\n"
     ]
    }
   ],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "rev_word_index = {idx:w for w,idx in word_index.items()}\n",
    "\n",
    "def decode_sentence(s):\n",
    "    # index 0 to 2 are reserved for things like padding, unknown word, etc.\n",
    "    decoded_sent = [rev_word_index.get(idx-3, '') for idx in s]\n",
    "    return ' '.join(decoded_sent)\n",
    "\n",
    "print(decode_sentence(x_train[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, hemos mostrado una crítica del conjunto de entrenamiento, la misma que mostramos anteriormente como vector de números.\n",
    "\n",
    "A continuación, vamos a decodificar todos los mensajes y a obtener un conjunto de entrenamiento decodificado, así como un conjunto de pruebas decodificado. Para ello, vamos a usar la función `decode_sentence` definida previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of crudely drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was foisted on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep adjusting the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the deluxe box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\n"
     ]
    }
   ],
   "source": [
    "x_train_decode = []\n",
    "for sentence in x_train:\n",
    "    decode_sent = decode_sentence(sentence)\n",
    "    x_train_decode.append(decode_sent)\n",
    "\n",
    "print(x_train_decode[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a quick glance at the premise of this film would seem to indicate just another dumb '80's inbred backwood slash fest the type where sex equals death and the actors are all annoying stereotypes you actually want to die however delivers considerably more br br rather than focus on bare flesh and gore though there is a little of each no sex however the flick focuses on delivering impending dread mounting tension amidst a lovely scenic backdrop these feelings are further heightened by a cast of realistically likable characters and antagonists that are more amoral than cardboard definitions of evil oh yeah george kennedy is here too and when is that not a good thing br br if you liked wrong turn then watch this to see where much of its' methodology came from\n"
     ]
    }
   ],
   "source": [
    "x_test_decode = []\n",
    "for sentence in x_test:\n",
    "    decode_sent = decode_sentence(sentence)\n",
    "    x_test_decode.append(decode_sent)\n",
    "\n",
    "print(x_test_decode[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es vectorizar los conjuntos de entrenamiento y pruebas, para ello, vamos a importar y utilizar `CountVectorizer` al igual que hicimos en ejercicios anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(x_train_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado el vectorizador, procedemos a obtener los conjuntos de entrenamiento y pruebas vectorizado. También, vamos a indicar las dimensiones de la matriz dispersa obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74702 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3445804 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "X_train = vect.transform(x_train_decode)\n",
    "X_test = vect.transform(x_test_decode)\n",
    "\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de entrenamiento tiene tantas filas como críticas o revisiones habia en el conjunto original y cuenta con 74.702 componentes, todos del mismo tamaño, que no es más que un vector de ceros y unos indicando cual de las palabras del corpus forman parte de dicha instancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, ya podemos comenzar el experimento utilizando `MLPClassifier` de scikit-learn. Para ello, vamos a crear el modelo y a entrenarlo directamente. Vamos a usar los parámetros por defecto salvo los que se modifican en la instancia del modelo, que son:\n",
    "* `solver`: vamos a mantener el mismo algoritmo de optimización de los pesos que en el ejercicio visto en la asignatura, ya que estabamos en un problema de similares características, es decir, clasificación binaria.\n",
    "* `random_state`: ya se ha visto en ejercicios anteriores, es una forma de fijar la inizialización del modelo. De esta forma, siempre obtendremos el mismo resultado. Es algo que usaremos a lo largo de la práctica allí donde se pueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento en entenamiento: 0.99\n",
      "Rendimiento en el conjunto de prueba: 0.86\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print(\"Rendimiento en entenamiento: {:.2f}\".format(mlp.score(X_train, y_train)))\n",
    "print(\"Rendimiento en el conjunto de prueba: {:.2f}\".format(mlp.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a modificar el parámetro `alpha` para ver si obtenemos un resultado diferente del anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento en entenamiento: 0.99\n",
      "Rendimiento en el conjunto de prueba: 0.85\n"
     ]
    }
   ],
   "source": [
    "mlp_alpha_1 = MLPClassifier(solver='lbfgs', alpha=1, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print(\"Rendimiento en entenamiento: {:.2f}\".format(mlp_alpha_1.score(X_train, y_train)))\n",
    "print(\"Rendimiento en el conjunto de prueba: {:.2f}\".format(mlp_alpha_1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 00:39:05.669098: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 300), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 7s 10ms/step - loss: 0.6684 - accuracy: 0.6874 - val_loss: 0.5671 - val_accuracy: 0.7332\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.4946 - accuracy: 0.7879 - val_loss: 0.4707 - val_accuracy: 0.7880\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4260 - accuracy: 0.8216 - val_loss: 0.5683 - val_accuracy: 0.7480\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3882 - accuracy: 0.8424 - val_loss: 0.3766 - val_accuracy: 0.8396\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3472 - accuracy: 0.8586 - val_loss: 0.3354 - val_accuracy: 0.8636\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.3276 - accuracy: 0.8691 - val_loss: 0.3586 - val_accuracy: 0.8530\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.3134 - accuracy: 0.8752 - val_loss: 0.3236 - val_accuracy: 0.8692\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.2811 - accuracy: 0.8865 - val_loss: 0.3076 - val_accuracy: 0.8766\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.2724 - accuracy: 0.8931 - val_loss: 0.3064 - val_accuracy: 0.8748\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.2588 - accuracy: 0.8978 - val_loss: 0.3140 - val_accuracy: 0.8780\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.2361 - accuracy: 0.9062 - val_loss: 0.3846 - val_accuracy: 0.8446\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.2324 - accuracy: 0.9068 - val_loss: 0.3078 - val_accuracy: 0.8722\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.2219 - accuracy: 0.9107 - val_loss: 0.3703 - val_accuracy: 0.8468\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.1988 - accuracy: 0.9204 - val_loss: 0.3104 - val_accuracy: 0.8798\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.1916 - accuracy: 0.9252 - val_loss: 0.3011 - val_accuracy: 0.8752\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.1802 - accuracy: 0.9309 - val_loss: 0.3105 - val_accuracy: 0.8748\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.1730 - accuracy: 0.9343 - val_loss: 0.3128 - val_accuracy: 0.8790\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.1553 - accuracy: 0.9413 - val_loss: 0.3052 - val_accuracy: 0.8832\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 15s 23ms/step - loss: 0.1582 - accuracy: 0.9398 - val_loss: 0.2989 - val_accuracy: 0.8754\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.1594 - accuracy: 0.9412 - val_loss: 0.3170 - val_accuracy: 0.8834\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.1393 - accuracy: 0.9489 - val_loss: 0.3229 - val_accuracy: 0.8784\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.1305 - accuracy: 0.9551 - val_loss: 0.3325 - val_accuracy: 0.8804\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.1200 - accuracy: 0.9596 - val_loss: 0.5941 - val_accuracy: 0.8144\n",
      "Epoch 24/30\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.1033 - accuracy: 0.9666 - val_loss: 0.3260 - val_accuracy: 0.8794\n",
      "Epoch 25/30\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.1131 - accuracy: 0.9655 - val_loss: 0.4157 - val_accuracy: 0.8580\n",
      "Epoch 26/30\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.0989 - accuracy: 0.9689 - val_loss: 2.0346 - val_accuracy: 0.6054\n",
      "Epoch 27/30\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.0846 - accuracy: 0.9744 - val_loss: 0.3918 - val_accuracy: 0.8790\n",
      "Epoch 28/30\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.0665 - accuracy: 0.9787 - val_loss: 0.4010 - val_accuracy: 0.8810\n",
      "Epoch 29/30\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.0797 - accuracy: 0.9768 - val_loss: 0.3619 - val_accuracy: 0.8710\n",
      "Epoch 30/30\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 0.0695 - accuracy: 0.9809 - val_loss: 0.3899 - val_accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 9s 11ms/step - loss: 0.3997 - accuracy: 0.8723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3997378647327423, 0.8722800016403198]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: aplicación de redes neuronales a regresión (predicción del precio de la vivienda)\n",
    "\n",
    "Se pide aplicar un modelo de redes neuronales al problema de predecir precios de vivienda usando el conjunto de datos  `Boston house prices`. \n",
    "\n",
    "Hacerlo usando los dos sistemas vistos, comparando los resultados:\n",
    "* Scikit learn: usar `MLPRegressor`\n",
    "* Keras con Tensorflow: usar nuevamente `Sequential` y capas tipo `Dense` con la arquitectura adecuada.\n",
    "\n",
    "El conjunto de datos se puede cargar usando tanto scikit learn (`sklearn.datasets.load_boston`) como keras (`keras.datasets.boston_housing`). \n",
    "\n",
    "Como en la parte 1, se pide mostrar algunas pruebas de los resultados obtenidos usando distintas arquitecturas y/o hiperparámetros. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
